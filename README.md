# FM
Наивная реализация Factorization Machine


### Структура репозитория

#### Materials:
> Статьи по теме

#### src:
> Исходный код и makefile

#### utils: 
> Python-скрипты для экспериментов и конвертации форматов(чисто для удобства)

На текущий момент есть:
* Парсинг vw формата, упрощённый вариант: у метки нет веса и лейбла, один namespace, фиче должно соответствовать float значение
* Матрица признаков хранится в OHE формате
* Две модели - линейная регрессия и FM
* Две функции потерь - mse и logloss
* Оптимизация только с помощью online(батч размера 1) SGD


### Используемые в коде абстракции

#### Классы:
> SparseVector, X, Y - хранят выборку и метки

> DataReader - считывает входной файл в оперативную память, для OHE составляют пары фича-индекс(для хеширования препроцесса входного файла не будет)

> Optimizer - абстракция для метода оптимизации, сейчас реализован только SGD

> Model - абстракция для модели, предоставляет интерфейс для подсчета градиента и предикта, реализованы две модели - линейная и FM

> Loss - абстракция для функции потерь, предоставляет интерфейс для подсчета лосса и градиента

> SparseWeights и Weights - абстракция для весов, отвечает за подсчет update'а в методе оптимизации и обновление весов модели

#### Вопросы по реализации:
> Для sparse-векторов сейчас используется по-сути list, пробовал использовать map и unordered_map - получал x10 замедление
на обучении линейной регресии. Возможно, стоит реализовать этот класс эффективнее, потому что для более сложных методов оптимизации будет важна производительность арифметических операций с sparse-векторами.

> Реализация с помощью абстрактных классов получается не совсем удачная, возможно, стоит переписать логику на шаблонах.


### ToDo 

В техническом плане
* Feature hashing(альтернатива для OHE)
* Dump модели
* Запись predictions в текстовый файл
* Ускорить парсинг входного файла(сейчас построчное чтение из потока - медленно)
* Арифметика над sparse-векторами(для размера батча >1)
* Добавление абстракции в класс Optimizer для других методов

В алгоритмическом плане:
* Регуляризации
* Кеширование(?) скалярных произведений(более общё - ускорение логики вычислений)
* Инициализация весов
* Более хитрые sparse-векторы

В содержательном плане:
* ALS и какой-нибудь адаптивный метод(SVRG, например)
* Hogwild(если успею)
* FFM или BFM(если успею)


### Пример использования

cd src

make all

./main --data ../../datasets/rcv1/rcv1.vw --test ../../datasets/rcv1/rcv1.test.vw --model fm --loss logistic --factors_size 5 --use_offset --passes 10 --learning_rate 0.01


### Сравнение c vw

Для примерочного сравнения я выбрал датасет rcv1, потому что в нём 700к примеров(достаточно много, чтобы проверка несла какую-то информацию, и достаточно мало, чтобы быстро получать результат), есть категориальные и численные фичи, он доступен в vw формате. 

Обучал и в случае vw, и в случае своей реализации 10 итераций обычного sgd с mse-loss и learning_rate=0.1. 

Для vw получилось ~0.042 на train и ~0.044 на test.
Для моей линейной регрессии ~0.054 на train и ~0.063 на test.
Для моей FM ~0.051 и ~0.136 на test.

Сейчас линейная модель(без учета чтения данных) тратит на 10 итераций SGD порядка 10сек, а FM c 5 факторами - 180сек, что много, требуется оптимизация кода.

Понятно, что FM даёт прирост качества на train, но переобучается, поэтому проводить более подробный анализ без регуляризации смысла нет. Так же для FM требуется некоторая инициализация весов(с нулевыми не обучается, с неправильными константами расходится) и достаточно маленький lr(я брал 0.01, c 0.1 расходится). Это решается адаптивными методами и более аккуратной инициализацией весов.
